diff --git a/src/cafaeval/__main__.py b/src/cafaeval/__main__.py
index f10984c..2064723 100644
--- a/src/cafaeval/__main__.py
+++ b/src/cafaeval/__main__.py
@@ -1,8 +1,9 @@
 import argparse
+import sys
+sys.path.append("/home/rashika/CAFA4/CAFA-evaluator/src/")
 from cafaeval.evaluation import cafa_eval, write_results
 import logging
 
-
 def command_line():
 
     logging.info("CAFA-evaluator. Calculate precision-recall curves and F-max / S-min")
@@ -36,6 +37,9 @@ def command_line():
                              'Do not use multithread if you are short in memory')
     parser.add_argument('-log_level', type=str, choices=['debug', 'info', 'warning', 'error', 'critical'],
                         default='info', help='Log level')
+    # Add argument for bootstrapping
+    parser.add_argument('-b', '--B', type=int, default=10, help='The number of times boostrapping should be done to calculate the confidence intervals for the evaluated metric')
+
 
     args = parser.parse_args()
 
@@ -50,8 +54,10 @@ def command_line():
     # Run the evaluation
     df, dfs_best = cafa_eval(args.obo_file, args.pred_dir, args.gt_file,
                              ia=args.ia, no_orphans=args.no_orphans, norm=args.norm, prop=args.prop,
-                             max_terms=args.max_terms, th_step=args.th_step, n_cpu=args.threads)
+                             max_terms=args.max_terms, th_step=args.th_step, n_cpu=args.threads, B = args.B)
 
+    print(df)
+    print(df.columns)
     # Write the results
     write_results(df, dfs_best, out_dir=args.out_dir, th_step=args.th_step)
 
diff --git a/src/cafaeval/evaluation.py b/src/cafaeval/evaluation.py
index c3f558e..a7a097c 100644
--- a/src/cafaeval/evaluation.py
+++ b/src/cafaeval/evaluation.py
@@ -151,7 +151,7 @@ def evaluate_prediction(prediction, gt, ontologies, tau_arr, normalization='cafa
     return dfs
 
 
-def cafa_eval(obo_file, pred_dir, gt_file, ia=None, no_orphans=False, norm='cafa', prop='max', max_terms=None, th_step=0.01, n_cpu=1):
+def cafa_eval(obo_file, pred_dir, gt_file, ia=None, no_orphans=False, norm='cafa', prop='max', max_terms=None, th_step=0.01, n_cpu=1, B = 0):
 
     # Tau array, used to compute metrics at different score thresholds
     tau_arr = np.arange(th_step, 1, th_step)
@@ -181,6 +181,9 @@ def cafa_eval(obo_file, pred_dir, gt_file, ia=None, no_orphans=False, norm='cafa
             df_pred['filename'] = file_name.replace(pred_folder, '').replace('/', '_')
             dfs.append(df_pred)
             logging.info("Prediction: {}, evaluated".format(file_name))
+        
+        if B and prediction:
+            print(prediction['cellular_component'])
 
     # Concatenate all dataframes and save them
     df = None
@@ -205,6 +208,8 @@ def cafa_eval(obo_file, pred_dir, gt_file, ia=None, no_orphans=False, norm='cafa
     else:
         logging.info("No predictions evaluated")
 
+    print(dfs_best)
+    
     return df, dfs_best
 
 
@@ -221,4 +226,10 @@ def write_results(df, dfs_best, out_dir='results', th_step=0.01):
     df.to_csv('{}/evaluation_all.tsv'.format(out_folder), float_format="%.{}f".format(decimals), sep="\t")
 
     for metric in dfs_best:
-        dfs_best[metric].to_csv('{}/evaluation_best_{}.tsv'.format(out_folder, metric), float_format="%.{}f".format(decimals), sep="\t")
\ No newline at end of file
+        dfs_best[metric].to_csv('{}/evaluation_best_{}.tsv'.format(out_folder, metric), float_format="%.{}f".format(decimals), sep="\t")
+        
+
+        
+#def bootstrap(df, B):
+        
+
